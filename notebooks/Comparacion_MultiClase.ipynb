{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9fb1a6f-f874-4a72-8016-ec7af3d3b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from networkx.linalg.graphmatrix import *\n",
    "from networkx.algorithms.approximation import *\n",
    "#from networkx.centrality import closn\n",
    "from networkx.algorithms import *\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86c0e6c-e27d-4ac9-a971-88775888d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath('C:\\\\Users\\\\Matteo\\\\Documents\\\\work in progress\\\\GRAPH_NN\\\\XaiGraph')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ad4d4-9192-49f3-9b12-a833643d30c2",
   "metadata": {},
   "source": [
    "# DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f231c8-92a2-47c4-b543-be42a10a2495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matteo\\matteo_virtualenv\\graph_nn\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "C:\\Users\\Matteo\\Documents\\work in progress\\GRAPH_NN\\XaiGraph\\datasets\\titanic_cooking.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['family_size'] = dataset['parch'] + dataset['sibsp']\n",
      "C:\\Users\\Matteo\\Documents\\work in progress\\GRAPH_NN\\XaiGraph\\datasets\\titanic_cooking.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['is_alone'] = 1\n",
      "C:\\Users\\Matteo\\matteo_virtualenv\\graph_nn\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\Matteo\\Documents\\work in progress\\GRAPH_NN\\XaiGraph\\datasets\\titanic_cooking.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['title'] = dataset['name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
      "C:\\Users\\Matteo\\Documents\\work in progress\\GRAPH_NN\\XaiGraph\\datasets\\titanic_cooking.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['title'] = dataset.title.apply(lambda x: 'rare' if rare_titles[x] else x)\n",
      "C:\\Users\\Matteo\\Documents\\work in progress\\GRAPH_NN\\XaiGraph\\datasets\\titanic_cooking.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_prepro[col_name][\n",
      "C:\\Users\\Matteo\\Documents\\work in progress\\GRAPH_NN\\XaiGraph\\datasets\\titanic_cooking.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_prepro[col_name][\n",
      "C:\\Users\\Matteo\\Documents\\work in progress\\GRAPH_NN\\XaiGraph\\datasets\\titanic_cooking.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_prepro[col_name][\n",
      "C:\\Users\\Matteo\\Documents\\work in progress\\GRAPH_NN\\XaiGraph\\datasets\\titanic_cooking.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_prepro[col_name][\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "    age  count\n",
      "0  0.85    128\n",
      "1  0.15    119\n",
      "2  0.55    117\n",
      "3  0.45    109\n",
      "4  0.75    106\n",
      "5  0.25    102\n",
      "6  0.35    101\n",
      "7  0.05     96\n",
      "8  0.65     90\n",
      "9  0.95     79\n",
      "TEST\n",
      "    age  count\n",
      "0  0.85     36\n",
      "1  0.55     30\n",
      "2  0.35     28\n",
      "3  0.75     28\n",
      "4  0.25     26\n",
      "5  0.65     24\n",
      "6  0.95     24\n",
      "7  0.05     23\n",
      "8  0.45     23\n",
      "9  0.15     20\n",
      "TRAIN\n",
      "   fare  count\n",
      "0  0.15    199\n",
      "1  0.65    145\n",
      "2  0.85    145\n",
      "3  0.25    123\n",
      "4  0.35     97\n",
      "5  0.45     89\n",
      "6  0.75     86\n",
      "7  0.55     83\n",
      "8  0.95     55\n",
      "9  0.05     25\n",
      "TEST\n",
      "   fare  count\n",
      "0  0.65     47\n",
      "1  0.15     44\n",
      "2  0.85     43\n",
      "3  0.45     24\n",
      "4  0.25     23\n",
      "5  0.55     23\n",
      "6  0.75     21\n",
      "7  0.95     16\n",
      "8  0.35     13\n",
      "9  0.05      8\n",
      "TRAIN\n",
      "   family_size  count\n",
      "0          0.0    643\n",
      "1          1.0    178\n",
      "2          2.0    123\n",
      "3          3.0     35\n",
      "4          4.0     20\n",
      "5          5.0     20\n",
      "6          6.0     15\n",
      "7         10.0      7\n",
      "8          7.0      6\n",
      "TEST\n",
      "   family_size  count\n",
      "0          0.0    147\n",
      "1          1.0     57\n",
      "2          2.0     36\n",
      "3          3.0      8\n",
      "4          5.0      5\n",
      "5         10.0      4\n",
      "6          4.0      2\n",
      "7          7.0      2\n",
      "8          6.0      1\n",
      "TRAIN\n",
      "  embarked  count\n",
      "0        S    730\n",
      "1        C    214\n",
      "2        Q    103\n",
      "TEST\n",
      "  embarked  count\n",
      "0        S    186\n",
      "1        C     56\n",
      "2        Q     20\n",
      "TRAIN\n",
      "      sex  count\n",
      "0    male    675\n",
      "1  female    372\n",
      "TEST\n",
      "      sex  count\n",
      "0    male    168\n",
      "1  female     94\n",
      "TRAIN\n",
      "   pclass  count\n",
      "0     3.0    578\n",
      "1     1.0    249\n",
      "2     2.0    220\n",
      "TEST\n",
      "   pclass  count\n",
      "0     3.0    131\n",
      "1     1.0     74\n",
      "2     2.0     57\n",
      "TRAIN\n",
      "    title  count\n",
      "0      Mr    606\n",
      "1     Mrs    366\n",
      "2  Master     51\n",
      "3    rare     24\n",
      "TEST\n",
      "    title  count\n",
      "0      Mr    151\n",
      "1     Mrs     91\n",
      "2  Master     10\n",
      "3    rare     10\n",
      "TRAIN\n",
      "   is_alone  count\n",
      "0         1    821\n",
      "1         0    226\n",
      "TEST\n",
      "   is_alone  count\n",
      "0         1    204\n",
      "1         0     58\n",
      "TRAIN\n",
      "  target  count\n",
      "0      0    647\n",
      "1      1    400\n",
      "TEST\n",
      "  target  count\n",
      "0      0    162\n",
      "1      1    100\n"
     ]
    }
   ],
   "source": [
    "from datasets.titanic_cooking import titanic_cooking\n",
    "\n",
    "X_train_prepro, X_test_prepro = titanic_cooking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d8203e-4703-4b21-acac-ff8cbad3a727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>family_size</th>\n",
       "      <th>embarked</th>\n",
       "      <th>sex</th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>is_alone</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "      <td>female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Master</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1047 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fare  family_size embarked     sex  target   title  is_alone  \\\n",
       "0     0.75  0.15          0.0        Q  female     3.0     Mrs         1   \n",
       "1     0.35  0.65          1.0        C  female     2.0     Mrs         1   \n",
       "2     0.05  0.75          6.0        S  female     3.0     Mrs         0   \n",
       "3     0.35  0.15          0.0        C    male     3.0      Mr         1   \n",
       "4     0.15  0.15          0.0        S  female     3.0     Mrs         1   \n",
       "...    ...   ...          ...      ...     ...     ...     ...       ...   \n",
       "1042  0.85  0.15          1.0        S  female     3.0     Mrs         1   \n",
       "1043  0.05  0.75          5.0        S    male     3.0  Master         0   \n",
       "1044  0.15  0.25          2.0        S    male     3.0      Mr         0   \n",
       "1045  0.15  0.35          0.0        S    male     2.0      Mr         1   \n",
       "1046  0.25  0.25          0.0        S    male     3.0      Mr         1   \n",
       "\n",
       "      survived  \n",
       "0          1.0  \n",
       "1          1.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          1.0  \n",
       "...        ...  \n",
       "1042       1.0  \n",
       "1043       0.0  \n",
       "1044       0.0  \n",
       "1045       0.0  \n",
       "1046       0.0  \n",
       "\n",
       "[1047 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prepro = X_train_prepro.rename(columns={\"target\": \"survived\", \"pclass\": \"target\"})\n",
    "X_train_prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a711a3a6-682b-4aa7-92f0-6128fc45cf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>family_size</th>\n",
       "      <th>embarked</th>\n",
       "      <th>sex</th>\n",
       "      <th>title</th>\n",
       "      <th>is_alone</th>\n",
       "      <th>survived</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Master</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1047 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fare  family_size embarked     sex   title  is_alone  survived  \\\n",
       "0     0.75  0.15          0.0        Q  female     Mrs         1       1.0   \n",
       "1     0.35  0.65          1.0        C  female     Mrs         1       1.0   \n",
       "2     0.05  0.75          6.0        S  female     Mrs         0       0.0   \n",
       "3     0.35  0.15          0.0        C    male      Mr         1       0.0   \n",
       "4     0.15  0.15          0.0        S  female     Mrs         1       1.0   \n",
       "...    ...   ...          ...      ...     ...     ...       ...       ...   \n",
       "1042  0.85  0.15          1.0        S  female     Mrs         1       1.0   \n",
       "1043  0.05  0.75          5.0        S    male  Master         0       0.0   \n",
       "1044  0.15  0.25          2.0        S    male      Mr         0       0.0   \n",
       "1045  0.15  0.35          0.0        S    male      Mr         1       0.0   \n",
       "1046  0.25  0.25          0.0        S    male      Mr         1       0.0   \n",
       "\n",
       "      target  \n",
       "0        3.0  \n",
       "1        2.0  \n",
       "2        3.0  \n",
       "3        3.0  \n",
       "4        3.0  \n",
       "...      ...  \n",
       "1042     3.0  \n",
       "1043     3.0  \n",
       "1044     3.0  \n",
       "1045     2.0  \n",
       "1046     3.0  \n",
       "\n",
       "[1047 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prepro = X_train_prepro[['age', 'fare', 'family_size', 'embarked', 'sex', 'title', 'is_alone', 'survived', 'target']]\n",
    "X_train_prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820365c0-4371-4d13-a928-d1fdecc22b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>family_size</th>\n",
       "      <th>embarked</th>\n",
       "      <th>sex</th>\n",
       "      <th>title</th>\n",
       "      <th>is_alone</th>\n",
       "      <th>survived</th>\n",
       "      <th>target_1.0</th>\n",
       "      <th>target_2.0</th>\n",
       "      <th>target_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Master</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1047 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fare family_size embarked     sex   title is_alone survived  \\\n",
       "0     0.75  0.15         0.0        Q  female     Mrs        1      1.0   \n",
       "1     0.35  0.65         1.0        C  female     Mrs        1      1.0   \n",
       "2     0.05  0.75         6.0        S  female     Mrs        0      0.0   \n",
       "3     0.35  0.15         0.0        C    male      Mr        1      0.0   \n",
       "4     0.15  0.15         0.0        S  female     Mrs        1      1.0   \n",
       "...    ...   ...         ...      ...     ...     ...      ...      ...   \n",
       "1042  0.85  0.15         1.0        S  female     Mrs        1      1.0   \n",
       "1043  0.05  0.75         5.0        S    male  Master        0      0.0   \n",
       "1044  0.15  0.25         2.0        S    male      Mr        0      0.0   \n",
       "1045  0.15  0.35         0.0        S    male      Mr        1      0.0   \n",
       "1046  0.25  0.25         0.0        S    male      Mr        1      0.0   \n",
       "\n",
       "     target_1.0 target_2.0 target_3.0  \n",
       "0             0          0          1  \n",
       "1             0          1          0  \n",
       "2             0          0          1  \n",
       "3             0          0          1  \n",
       "4             0          0          1  \n",
       "...         ...        ...        ...  \n",
       "1042          0          0          1  \n",
       "1043          0          0          1  \n",
       "1044          0          0          1  \n",
       "1045          0          1          0  \n",
       "1046          0          0          1  \n",
       "\n",
       "[1047 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cols = list(X_train_prepro.columns)[:-1]\n",
    "x = X_train_prepro[x_cols].values\n",
    "y = pd.get_dummies(X_train_prepro.target, prefix='target')\n",
    "y_cols = list(y.columns)\n",
    "y = y.values\n",
    "final_df = pd.DataFrame(np.concatenate((x,y), axis=1), columns = x_cols + y_cols)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e0404d4-831e-4e4e-b6ad-af7d976dfd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>family_size</th>\n",
       "      <th>embarked</th>\n",
       "      <th>sex</th>\n",
       "      <th>title</th>\n",
       "      <th>is_alone</th>\n",
       "      <th>survived</th>\n",
       "      <th>target_1.0</th>\n",
       "      <th>target_2.0</th>\n",
       "      <th>target_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Master</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>rare</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>C</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.0</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>561 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  fare  family_size embarked     sex   title  is_alone  survived  \\\n",
       "0    0.05  0.05          1.0        S    male  Master         1       1.0   \n",
       "1    0.05  0.15          0.0        C  female     Mrs         1       1.0   \n",
       "2    0.05  0.15          0.0        S  female     Mrs         1       0.0   \n",
       "3    0.05  0.15          2.0        C    male      Mr         0       0.0   \n",
       "4    0.05  0.25          0.0        Q  female     Mrs         1       1.0   \n",
       "..    ...   ...          ...      ...     ...     ...       ...       ...   \n",
       "556  0.95  0.85          2.0        S    male    rare         0       1.0   \n",
       "557  0.95  0.95          1.0        S  female     Mrs         1       1.0   \n",
       "558  0.95  0.95          2.0        S    male      Mr         0       0.0   \n",
       "559  0.95  0.95          4.0        C    male      Mr         0       0.0   \n",
       "560  0.95  0.95          5.0        S    male      Mr         0       0.0   \n",
       "\n",
       "     target_1.0  target_2.0  target_3.0  \n",
       "0           0.0         0.0         1.0  \n",
       "1           0.0         0.0         1.0  \n",
       "2           0.0         0.0         1.0  \n",
       "3           0.0         0.0         1.0  \n",
       "4           0.0         0.0         1.0  \n",
       "..          ...         ...         ...  \n",
       "556         1.0         0.0         0.0  \n",
       "557         1.0         0.0         0.0  \n",
       "558         1.0         0.0         0.0  \n",
       "559         1.0         0.0         0.0  \n",
       "560         1.0         0.0         0.0  \n",
       "\n",
       "[561 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(final_df.columns)\n",
    "df_2_explain= (final_df\n",
    "               .groupby([c for c in cols if \"target\" not in c])\n",
    "               .agg({c: ['mean'] for c in cols if 'target' in c})\n",
    "               .reset_index())\n",
    "df_2_explain.columns = ['_'.join(col).strip() if col[1] != '' else col[0] for col in df_2_explain.columns.values]\n",
    "df_2_explain = df_2_explain.rename(columns={'_'.join([c, 'mean']): c for c in cols if 'target' in c})\n",
    "df_2_explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91445d7-7124-4b15-a08d-8eafd46355ec",
   "metadata": {},
   "source": [
    "# TEF SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53082d-48f9-4846-8e77-af5ce19234de",
   "metadata": {},
   "source": [
    "## REQUISITOS DATASET DE ENTRADA\n",
    "\n",
    "El dataset de entrada utilizado por el algoritmo de explicabilidad local TEF-SHAP tiene que tener las siguientes caracteristicas:\n",
    "\n",
    "* **Número de columnas (features)**: Por tema computacional, cuando el número de columnas del dataset es >= 20, es actualmente necesario pre-seleccionar un número de features top_k a priori más relevantes a tener en cuenta para la explicabilidad. El número de variables más relevantes tiene que ser top_k <= 20 (siendo 10 el número recomendado). Se está trabajando sobre posibles implementaciones que no tengan esta limitación computacional, no obstante la versión actual del algoritmo se espera que quien proporciona el dataset (usuario de la librería) se encargue de seleccionar las top_k columnas en base a algún tipo de heuristica previa (por ejemplo estudio de correlación, estudio explicabilidad global, etc.). NOTA: Una vez que se haya determinado en base a alguna heuristica las top_k variables a priori más relevantes, veáse la sección sobre \"eliminación de variables menos relevantes\" para una guia sobre los pasos a seguir para eliminar del datasets las variables menos relevantes. \n",
    "* **Columnas (features) categóricas**: Por construcción, las variables categóricas asumen un número finito y discreto de valores. IMPORTANTE: ¡NO hay que pasar las variables categoricas a One-Hot-Encoding! En el caso en que el dataset original contenga variable OHE, es necesario des-hacer estas variables y recuperar la variable categórica original. NOTA: Es aconsejable que el número de posibles valores de las variables categóricas sea limitado. En los casos de variables categóricas con > 15-20 valores posibles, es aconsejable aplicar algún tipo de trasformación previa para reducir la dimensionalidad de esta variable categórica.  \n",
    "* **Columnas (features) númericas**: Es necesario discretizar todas las variables númericas. En principio cualquier mecanismo de discretización es válido (Cuantiles, sampling despues de DistFit, reglas de negocio, etc.). Es muy aconsejable por tema computaciónal reducir al máximo el número de valores discretos (<=10 por ejemplo).\n",
    "* **Columna Target**: El algoritmo se espera una sola columna de target que asuma valores float. Puede ser (0.=False,1.=True) o puede ser cualquier valor continuo. A priori el target no tiene que estar agotado a un rango preciso. IMPORTANTE: El algoritmo de explicación local TEF-SHAP está pensado para problemas de regresión o problema de clasificación binarios. Vease la sección \"Explicación multiclasificador\" para los pasos a seguir para el caso de querer explicar la predicción de un multiclasificador.\n",
    "\n",
    "A continuación se muestran varios ejemplos de posibles entrada del algoritmo TEF-SHAP que cumplen con los requisitos anteriormente descritos.\n",
    "\n",
    "**EJEMPLO 1: Clasificador binario, target=probabilidad**\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target |\n",
    "|---------|--------|------|-----------|--------|\n",
    "| 35      | 100    | H    | 1001      | 0.9    | \n",
    "| 45      | 90     | H    | 1002      | 0.3    |\n",
    "| 60      | 50     | M    | 1003      | 0.1    |\n",
    "| 30      | 50     | M    | Otros     | 0.77   |\n",
    "\n",
    "donde:\n",
    "* La variable continua Edad se ha discretizado redondeando a un múltiplo de 5\n",
    "* La variable continua Revenue se ha discretizado con 10 cuantiles\n",
    "* La variable categórica Sexo no se ha tocado\n",
    "* La variable id_tarifa que asumía > 1000 valores se ha agrupado en N<=20 valores\n",
    "* Target es una variable entre 0 y 1\n",
    "\n",
    "**EJEMPLO 2: Clasificador binario, target=clase**\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target |\n",
    "|---------|--------|------|-----------|--------|\n",
    "| 35      | 100    | H    | 1001      | 1      | \n",
    "| 45      | 90     | H    | 1002      | 0      |\n",
    "| 60      | 50     | M    | 1003      | 0      |\n",
    "| 30      | 50     | M    | Otros     | 1      |\n",
    "\n",
    "donde:\n",
    "* La variable continua Edad se ha discretizado redondeando a un múltiplo de 5\n",
    "* La variable continua Revenue se ha discretizado con 10 cuantiles\n",
    "* La variable categórica Sexo no se ha tocado\n",
    "* La variable id_tarifa que asumía > 1000 valores se ha agrupado en N<=20 valores\n",
    "* Target es una variable binaria (pero en formato float) \n",
    "\n",
    "**EJEMPLO 3: Regressión, target=float**\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target |\n",
    "|---------|--------|------|-----------|--------|\n",
    "| 35      | 100    | H    | 1001      | 136    | \n",
    "| 45      | 90     | H    | 1002      | 1045   |\n",
    "| 60      | 50     | M    | 1003      | 39     |\n",
    "| 30      | 50     | M    | Otros     | -123   |\n",
    "\n",
    "donde:\n",
    "* La variable continua Edad se ha discretizado redondeando a un múltiplo de 5\n",
    "* La variable continua Revenue se ha discretizado con 10 cuantiles\n",
    "* La variable categórica Sexo no se ha tocado\n",
    "* La variable id_tarifa que asumía > 1000 valores se ha agrupado en N<=20 valores\n",
    "* Target es una variable continua (float) \n",
    "\n",
    "### Explicación multiclasificador\n",
    "\n",
    "En el caso de querer aplicar el algoritmo de explicabilidad local TEF-SHAP al caso de un multiclasificador con N clases, es necesario transformar el problema en N problemas binarios, siguiendo los siguientes pasos:\n",
    "\n",
    "**PREPROCESAMIENTO**\n",
    "* Seleccionar los top_k <= 20 variables a priori más relevantes. IMPORTANTE: ¡Las mismas top_K variables para todas las N clases que se quieren analizar!\n",
    "* Discretizar las features continuas\n",
    "* Preprocesar (si necesario) las features categóricas con > 15-20 niveles\n",
    "\n",
    "**TRANSFORMAR PROBLEMA MULTICLASE EN N PROBLEMAS BINARIOS**\n",
    "\n",
    "Podemos distinguir dos casos: \n",
    "\n",
    "CASO 1: TARGET=CLASE\n",
    "\n",
    "Dataset obtenido después del preprocesamiento:\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target |\n",
    "|---------|--------|------|-----------|--------|\n",
    "| 35      | 100    | H    | 1001      | Rojo   | \n",
    "| 45      | 90     | H    | 1002      | Verde  |\n",
    "| 60      | 50     | M    | 1003      | Verde  |\n",
    "| 30      | 50     | M    | Otros     | Azul   |\n",
    "\n",
    "N=3 Datsets a usar como entradas del algoritmo TEF-SHAP\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target=Rojo |\n",
    "|---------|--------|------|-----------|-------------|\n",
    "| 35      | 100    | H    | 1001      | 1           | \n",
    "| 45      | 90     | H    | 1002      | 0           |\n",
    "| 60      | 50     | M    | 1003      | 0           |\n",
    "| 30      | 50     | M    | Otros     | 0           |\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target=Verde |\n",
    "|---------|--------|------|-----------|--------------|\n",
    "| 35      | 100    | H    | 1001      | 0            | \n",
    "| 45      | 90     | H    | 1002      | 1            |\n",
    "| 60      | 50     | M    | 1003      | 1            |\n",
    "| 30      | 50     | M    | Otros     | 0            |\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target=Azul |\n",
    "|---------|--------|------|-----------|-------------|\n",
    "| 35      | 100    | H    | 1001      | 0           | \n",
    "| 45      | 90     | H    | 1002      | 0           |\n",
    "| 60      | 50     | M    | 1003      | 0           |\n",
    "| 30      | 50     | M    | Otros     | 1           |\n",
    "\n",
    "CASO 2: TARGET=ARRAY(PROBABILIDADES)\n",
    "\n",
    "Dataset obtenido después del preprocesamiento:\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target=Rojo | target=Verde | target=Azul |\n",
    "|---------|--------|------|-----------|-------------|--------------|-------------|\n",
    "| 35      | 100    | H    | 1001      | 0.8         | 0.15         | 0.05        | \n",
    "| 45      | 90     | H    | 1002      | 0.2         | 0.6          | 0.2         |\n",
    "| 60      | 50     | M    | 1003      | 0.3         | 0.68         | 0.02        |\n",
    "| 30      | 50     | M    | Otros     | 0.1         | 0.4          | 0.5         |\n",
    "\n",
    "N=3 Datsets a usar como entradas del algoritmo TEF-SHAP\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target=Rojo |\n",
    "|---------|--------|------|-----------|-------------|\n",
    "| 35      | 100    | H    | 1001      | 0.8         | \n",
    "| 45      | 90     | H    | 1002      | 0.2         |\n",
    "| 60      | 50     | M    | 1003      | 0.3         |\n",
    "| 30      | 50     | M    | Otros     | 0.1         |\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target=Verde |\n",
    "|---------|--------|------|-----------|--------------|\n",
    "| 35      | 100    | H    | 1001      | 0.15         | \n",
    "| 45      | 90     | H    | 1002      | 0.6          |\n",
    "| 60      | 50     | M    | 1003      | 0.68         |\n",
    "| 30      | 50     | M    | Otros     | 0.4          |\n",
    "\n",
    "| Edad    | Revenue| Sexo | id_tarifa | target=Azul |\n",
    "|---------|--------|------|-----------|-------------|\n",
    "| 35      | 100    | H    | 1001      | 0.05        | \n",
    "| 45      | 90     | H    | 1002      | 0.2         |\n",
    "| 60      | 50     | M    | 1003      | 0.02        |\n",
    "| 30      | 50     | M    | Otros     | 0.5         |\n",
    "\n",
    "\n",
    "### Carta a los Reyes Magos: Pipelines\n",
    "\n",
    "Sería deseable que la librería proporcionara al usuario herramientas para:\n",
    "\n",
    "* Seleccionar las features a priori más relevantes: Herramienta de explicabilidad global, estudio de correlaciones, etc.\n",
    "* Discretización de features númericas: cuantiles, distfit+sampleo, reglas de negocio, etc.\n",
    "* Transformación de features categóricas (necesario para reducir los niveles cuando la cardinalidad de la variable supera 15-20)\n",
    "* Pipeline: para hilar/automatizar todos los pasos anteriores\n",
    "\n",
    "### Eliminación de variables menos relevantes\n",
    "\n",
    "En el caso de un dataset con un número de columnas M >= 20 es actualmente necesario pre-seleccionar un número top_k < 20 (siendo 10 el aconsejable) de columnas a priori más importantes/revelantes. En este caso los pasos a seguir para generar el dataset de entrada del algoritmo son los siguientes: \n",
    "\n",
    "**PREPROCESAMIENTO**\n",
    "* Seleccionar los top_k <= 20 variables a priori más relevantes. IMPORTANTE: ¡Las mismas top_K variables para todas las N clases que se quieren analizar!\n",
    "* Discretizar las features continuas\n",
    "* Preprocesar (si necesario) las features categóricas con > 15-20 niveles\n",
    "\n",
    "**ELIMINAR LAS M-TOP_K FEATURES A PRIORI MENOS RELEVANTES**\n",
    "\n",
    "A partir del dataset obtenido como salida del preprocesamiento: \n",
    "\n",
    "| Edad (top_k) | Revenue (top_k) | Sexo (top_k) | id_tarifa (Eliminar) | target |\n",
    "|--------------|-----------------|--------------|----------------------|--------|\n",
    "| 35           | 100             | H            | 1001                 | 0      |\n",
    "| 35           | 100             | H            | 1004                 | 0      |\n",
    "| 35           | 100             | H            | 1005                 | 1      |\n",
    "| 45           | 90              | H            | 1002                 | 1      |\n",
    "| 45           | 90              | H            | 1006                 | 1      |\n",
    "| 60           | 50              | M            | 1003                 | 1      |\n",
    "| 60           | 50              | M            | 1007                 | 1      |\n",
    "| 60           | 50              | M            | 1008                 | 0      |\n",
    "| 30           | 50              | M            | Otros                | 0      |\n",
    "\n",
    "Calculamos el dataset que se obtiene haciendo:\n",
    "* groupBy(top_k Features a priori más relevantes)\n",
    "* mean(target)\n",
    "\n",
    "| Edad (top_k) | Revenue (top_k) | Sexo (top_k) | target |\n",
    "|--------------|-----------------|--------------|--------|\n",
    "| 35           | 100             | H            | 0.33   |\n",
    "| 45           | 90              | H            | 1      |\n",
    "| 60           | 50              | M            | 0.66   |\n",
    "| 30           | 50              | M            | 0      |\n",
    "\n",
    "Usamos este datset como entrada del algoritmo TEF-SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a7dd1-39b2-4ad7-8e04-31ab8f35bea3",
   "metadata": {},
   "source": [
    "## SHAP: Teoría\n",
    "\n",
    "NOTA: Prepararé una ppt con la teoría completa de SHAP. En este notebook se hace un resumen solo de las formulas necesarias para la implementación\n",
    "\n",
    "### Shapley values y SHAP\n",
    "\n",
    "La idea nace en el contexto de los juegos de coalición y fue desarrollada por Lloyd Shapley (1953) que fue galardonado con el premio Nobel por la economía en 2012.\n",
    "\n",
    "La idea es la siguiente: dada una coalición de jugadores (por ejemplo un conjunto de empleados) que consiguen un premio P (por ejemplo una facturación de 300K) averiguar como repartir de forma ecuatitava entre todos los jugadores el premio (la ganancia) teniendo en cuenta la aportación real de cada uno. Dicho de otras formas, queremos ser capaces de determinar de forma justa cuanto ha aportado (es decir la importancia de) cada miembro del equipo al resultado final.\n",
    "\n",
    "La aplicación al caso de modelo de Machine Learning es la siguiente. Hemos entrenado un modelo de Machine Learning $F(x)$ para resolver un problema de clasificación binaria (por ejemplo). Nuestro modelo toma de entrada $D$ features $x = [x_1, x_2, ..., x_D]$ y queremos determinar cuanto aporta (es decir la importancia de) cada feature $x_i$ en la predicción final $F(x)$ del modelo.  \n",
    "\n",
    "La idea base propuesta por Shapley para calcular la aportación de i-esima features es la siguiente: \n",
    "\n",
    "1. Crear todas las posibles combinaciones de jugadores (features) que en este contesto se denominan coaliciones\n",
    "2. Calcular la estimación promedio del modelo de Machine Learning $ E[F(x)] $\n",
    "3. Para cada coalición, calcular el valor la diferencia entre la predicción del modelo *SIN* la i-esima feature y la predicción promedia del punto 2\n",
    "4. Para cada coalición, calcular el valor la diferencia entre la predicción del modelo *CON* la i-esima feature y la predicción promedia del punto 2\n",
    "5. Para cada coalición, calcular la diferencia entre los valores calculados en los puntos 4 y 3, es decir calcular la contribución marginal de la i-esima features\n",
    "6. Shapley value = el promedio (sobre todas las posibles coaliciones) del valor calculado en el punto 5\n",
    "\n",
    "De una forma matemática, los puntos anteriores se pueden re-escribir de la siguiente manera\n",
    "\n",
    "$$\n",
    "\\phi_i (\\nu) = \\sum_{S \\in D - i} \\frac{|S|! (D - 1 - |S|)!}{D!} \\left[ \\nu(S \\, \\cup \\{i\\}) - \\nu(S) \\right]\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "* $\\phi_i$ es la importancia de la feature $i$, es decir cuanto contribuye la features $i$ en explicar la diferencia entre la predicción del modelo $F(X)$ y la baseline $E[F(x)]$. Vease la propiedad *Efficiency* abajo para la justificación matemática de esta definición.\n",
    "* $D$ es el número de features \n",
    "* $S$ es el conjunto de features (jugadores) que entran en la coalición\n",
    "* $|S|$ es el número de features (jugadores) que entran en la coalición\n",
    "* $\\sum_{S \\in D - i}$ es la suma de todas la coaliciones que se pueden construir a partir de las D features sin incluir la i-esima feature. **IMPORTANTE** esta suma tiene **$O\\left(2^D\\right)$ elementos**\n",
    "* $\\nu(S)$ es la función de merito de la coalición $S$. \n",
    "    * En el caso de un modelo de Machine Learning,  **SHAP** estima $\\nu(S)$ usando $\\nu(S) = E_{x_{\\bar S} | x_S} \\left[ F(x_S, x_{\\bar S}) \\right]$, es decir el valor esperado de la predicción del modelo condicionado por la presencia de las features de $x_S$ de la coalición $S$. Por claridad, $x_{\\bar S}$ son las features del dataset NO incluidas en la coalición.\n",
    "    * El valor de $\\nu(S)$ depende de la coalición pero NO del orden de las features. El factor $\\frac{|S|! (D - 1 - |S|)!}{D!}$ que aparece en la formula es el peso de cada coalición teniendo en cuenta el número de diferentes ordenaciones de las features que dan lugar a la misma coalición. Por ejemplo en el caso de un dataset con 5 features $f_1, f_2, f_3, f_4, f_5$ e $i=5$, la coalición $S=\\{f_1, f_2\\}$ se puede obtener a partir de las siguientes secuencias de features $seq = [(x_S=[f_1, f_2], x_{\\bar S}[f_3, f_4]), (x_S=[f_1, f_2], x_{\\bar S}[f_4, f_3]), (x_S=[f_2, f_1], x_{\\bar S}[f_3, f_4]), (x_S=[f_2, f_1], x_{\\bar S}[f_4, f_3])]$. El peso de la coalición $S$ es dado por $\\frac{|S|! (D - 1 - |S|)!}{D!} = \\frac{|2|! (5 - 1 - |2|)!}{5!} = len(seq)/5! = 4/5! = 1/30$.  \n",
    "    \n",
    "Se puede demonstrar que esta definición satisface los *Fairness Axioms*:\n",
    "* **Efficiency**: $\\sum_{i \\in D} \\phi_i (\\nu) = \\nu(D) - \\nu(0)$ que implica que la suma de los SHAP Values es igual a la diferencia entre el valor de la coalición completa (es decir la predicción del modelo incluyendo todas las features) y el valor de la coalición nula (es decir la predicción del modelo sin features que estimamos como el valor esperado de la predicción del modelo). Es importante notar que esta propiedad nos garantiza la accuracy local de SHAP:  \n",
    "\n",
    "$$F(x) = \\nu(D) = \\nu(0) + \\sum_{i \\in D} \\phi_i (\\nu) = \\nu(D) = E\\left[ F(x) \\right] + \\sum_{i \\in D} \\phi_i (\\nu)$$\n",
    "\n",
    "* **Symmetry**: Si dos features $(i,j)$ son intercambiables es decir si $\\nu(S \\cup \\{i\\}) = \\nu(S \\cup \\{j\\})$ para todas las coaliciones $S \\subseteq D$ entonces $\\phi_i(\\nu) = \\phi_j(\\nu)$\n",
    "* **Null Player**: Si $\\nu(S \\cup \\{i\\}) = \\nu(S)$ para todas las coaliciones $S \\subseteq D$ entonces $\\phi_i(\\nu) = 0$\n",
    "* **Linearity**: $\\phi(c_1 \\nu_1 + c_2 \\nu_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284d994-05e9-4f5f-b568-1cbc1cdb97c4",
   "metadata": {},
   "source": [
    "## Problemas relacionados con SHAP: $\\nu(S) = E_{x_{\\bar S} | x_S} \\left[ F(x_S, x_{\\bar S}) \\right]$\n",
    "\n",
    "En el caso de SHAP, la función de mérito se define como:\n",
    "\n",
    "$$\n",
    "\\nu(S) = E_{x_{\\bar S} | x_S} \\left[ F(x_S, x_{\\bar S}) \\right]\n",
    "$$\n",
    "\n",
    "Es decir las features $x_{\\bar S}$ que no pertenecen a la coalición $S$ se \"marginalize out\" (¿cómo se dice en castellano?) usando la \"conditional distribution\"\n",
    "\n",
    "$$\n",
    "E_{x_{\\bar S} | x_S} \\left[ F(x_S, x_{\\bar S}) \\right] = \\int_{x_{\\bar S}} F(x_S, x_{\\bar S}) P(x_{\\bar S} | x_S) dx_{\\bar S}\n",
    "$$\n",
    "\n",
    "En general estimar la \"conditional distribution\" no es una tarea sencilla y se suele aproximar la \"conditional distribution\" con la \"marginal distribution\"\n",
    "\n",
    "$$\n",
    "P(x_{\\bar S} | x_S) \\approx P(x_{\\bar S}) \n",
    "$$\n",
    "\n",
    "**NOTA**: Para el futuro cuanto aporta a nivel de calidad de resultados y cuanto ralentiza computacionalmente estimar la \"conditional distribution\" con los grafos? Idea: $Adj * ini$ donde $ini=x_S$.\n",
    "\n",
    "Usando la \"marginal distribution\", podemos re-escribir la función de merito de la siguiente manera \n",
    "\n",
    "$$\n",
    "E_{x_{\\bar S} | x_S} \\left[ F(x_S, x_{\\bar S}) \\right] \\approx \\int_{x_{\\bar S}} F(x_S, x_{\\bar S}) P(x_{\\bar S}) dx_{\\bar S}\n",
    "$$\n",
    "\n",
    "La última expresión puede ser facilmente estimada usando Monte Carlo. Seleccionamos $m$ del dataset y aproximamos\n",
    "\n",
    "$$\n",
    "E_{x_{\\bar S} | x_S} \\left[ F(x_S, x_{\\bar S}) \\right] \\approx \\sum_{x_{\\bar S}} F(x_S, x_{\\bar S}) P(x_{\\bar S}) \\approx \\frac{1}{m} \\sum_{i=1}^{m} F(x_S^i, x_{\\bar S}^i)\n",
    "$$\n",
    "\n",
    "Importante notar que para esta estimación es necesario tener acceso a la función $F(x)$ del modelo. Uno de los requisitos del proyecto XAIoGraph es que trabaje con cualquier dataset del tipo $(x_1, x_2, ..., x_D, y)$ sin tener acceso a la función $y=F(x_1, x_2, ..., x_D)$ que ha generado el target $y$. Por este motivo, dentro del proyecto XAIoGraph se propone estimar la función de mérito a partir de los datos a disposición:\n",
    "\n",
    "\n",
    "$$\n",
    "E_{x_{\\bar S} | x_S} \\left[ F(x_S, x_{\\bar S}) \\right] \\approx \\frac{1}{m} \\sum_{i: (x^i_S, x_{\\bar S}^i) = (x_S, x_{\\bar S})}^{m} F(x_S^i, x_{\\bar S}^i) = \\frac{1}{m} \\sum_{i: (x^i_S, x_{\\bar S}^i) = (x_S, x_{\\bar S})}^{m} y^i(x_S^i, x_{\\bar S}^i)\n",
    "$$\n",
    "\n",
    "En general, se considera una debilidad del algoritmo de explicabilidad si la explicación depende de los datos utilizados. No obstante, \n",
    "* Si el dataset que se proporciona como apoyo (~train) del modelo de explicabilidad es estadisticamente relevante (por ejemplo coincide con el dataset utilizado para el train del modelo a explicar) estas limitaciones deberían suavizarse.\n",
    "* (si se cumple el punto anterior) Estimar el valor esperado del target a partir de un dataset estadisticamente significativo podría suavizar el problema de  $P(x_{\\bar S} | x_S) \\approx P(x_{\\bar S})$ puesto que estaríamos usando los datos entrenamientos del modelo que se han considerado sufientes para caracterizar el fenomeno bajo estudio (es decir deberían capturar la $P(x_{\\bar S} | x_S)$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf5fb0-3ddb-4bae-9c93-736f3a5863b9",
   "metadata": {},
   "source": [
    "## Problemas relacionados con SHAP: Escalado con el número features\n",
    "\n",
    "La definición del Shapley value tiene un problema computacionalmente conocido:\n",
    "\n",
    "$$\n",
    "\\phi_i (\\nu) = \\sum_{S \\in D - i} \\frac{|S|! (D - 1 - |S|)!}{D!} \\left[ \\nu(S \\, \\cup \\{i\\}) - \\nu(S) \\right]\n",
    "$$\n",
    "\n",
    "Los terminos de las suma escalan como $O(D * 2^D)$ donde $D$ es el número de features del dataset. En el caso de querer calcular la explicabilidad local de un dataset de $K$ elementos la carga computacional sería $O(K*D*2^D)$. Teniendo en cuenta que $2^{20}=1.048.576$, en el caso de un dataset de 1M de registros estaríamos hablando de $O(10^{12})$ elementos en la suma de la formula anterior. \n",
    "\n",
    "Para suavizar este problema existen varias posibles soluciones: \n",
    "\n",
    "* Permutation-based estimation (Vease sección abajo)\n",
    "* Regression-based estimation (enfoque relacionado con LIME) (vease sección abajo)\n",
    "* FastSHAP: **TODO: Estudiar**\n",
    "* Seleccionar previamente top_k <= 20 columnas con otros argumentos (estudio correlación, otros mecanismos de explicabilidad global, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34ef0d-98bd-4410-8945-95bb5794d1a3",
   "metadata": {},
   "source": [
    "## TEF SHAP: Implementación exacta\n",
    "\n",
    "Problema computacional cuando el número de columnas >~20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d34f02a-6c3f-4ec9-82bf-4e44b5f289fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a396c74-6e51-446a-8931-dfbb61939a81",
   "metadata": {},
   "source": [
    "**FUNCIONES DE TRAIN**\n",
    "\n",
    "**get_coalitions**\n",
    "A partir de la lista de columnas del dataframe a analizar, esta función calcula todas las coaliciones de columnas compatibles con la columnas que recibe como parámetro de entrada. \n",
    "\n",
    "NOTA1: No se incluyen en la coaliciones ni la columna misma ni la columna target.\n",
    "\n",
    "NOTA2: Las posible coaliciones dependen de las columnas y no de las filas, de manera que es posible precalcularla y guardarlas en memoria. \n",
    "Por supuesto esto tiene un coste en memoria: un dataset de $D$ columnas da lugar a $2^{D-1}$ combinaciones para cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43ed6546-943f-428f-9a9a-d5d7c2318a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coalitions(columns: List[str], col:str, target_cols: Optional[List[str]]=['target']) -> List[List[str]]:\n",
    "    \"\"\"Given the list of dataframe columns, compute the list of coalitions of columns compatible with the column specified in the parameter\n",
    "    col\n",
    "    \n",
    "    :param columns: List of columns from which build the coalitions\n",
    "    :param col: str, Name of the column to exclude from the coalitions\n",
    "    :param target_cols: List[str], list with the names of the target columns. By default = ['target']\n",
    "    :return A list of coalitions compatible with the column col. Each coalition is itself a List \n",
    "    \"\"\"\n",
    "    remaining_features: List[str] = [feature for feature in columns if feature not in [col] + target_cols]\n",
    "    coalitions_list: List = []\n",
    "    for feature in range(len(remaining_features) + 1):\n",
    "        for coalition in combinations(remaining_features, feature):\n",
    "            coalitions_list.append(list(coalition))\n",
    "    return coalitions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67053ef2-35b6-4b3a-b698-2d8795721324",
   "metadata": {},
   "source": [
    "**train**\n",
    "Para intentar reducir tiempo computacional (a coste de un mayor uso de memoria), este método intenta pre-calcular unas serie de cantidades necesarias para la explicabilidad: \n",
    "* model: A partir del dataset de \"entrenamiento\" se cálcula el groupby.count por las columnas del dataframe. Es un dict de Python donde las keys son los nombres de las columnas y los values son numpy.array con los valores de las columnas.\n",
    "* coalitions: Un diccionario con las coaliciones compatibles con cada columna. Key: nombre columna, Value: lista de coaliciones compatibles\n",
    "* coalition weights: Un diccionario con los pesos (factorial) de las coaliciones. Key: nombre columna, Value: lista de pesos\n",
    "\n",
    "NOTA: El parámetro lens_to_exclude sirve para no incluir entre las coaliciones aquellas que tengan las longitudes especificadas en esta lista. Este mecánismo todavía no funciona correctamente, no usar de momento, es decir inicializar como lista vacía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c02ccf30-d41f-452f-993b-bce6ad827c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df: pd.DataFrame,  \n",
    "          target_cols:Optional[List[str]]=['target'], count_col:Optional[str]='count') -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"In order to reduce computational time (spending more memory), this function pre-compute\n",
    "       - groupby by feature columns in order to reduce the size of the dataset to use to estimate the \\nu(x)\n",
    "       - coalitions: a dictionary with the coalitions compatibles with each column. key: col name, value: list of coalitions\n",
    "       - coalition weights: the factorial weights of each coalitions. key: col name, value: list of coalition weiths\n",
    "    \n",
    "    :param df: pd.DataFrame to use as importance estimator, for example the dataset used to train the model to explain\n",
    "    :param lens_to_exclude: List: coalition lenght to not include in the computation. WARNING: It is working in progress, please don`t use it\n",
    "    :param target_col: str, target column names, by default='target'\n",
    "    :param count_col: str, target column names, by default='count'\n",
    "    :return Tuple -> model: Dict, coalitions: Dict, coalitions_weights: Dict\n",
    "    \"\"\"\n",
    "    # GroupBy features:\n",
    "    #  - count: peso de la tupla de features\n",
    "    #  - (target) mean: promedio del target para una tupla de features\n",
    "    # Nota: el modelo original a explicar trabaja con feature númericas continuas y con features categóricas potencialmente con mucho niveles\n",
    "    #       para reducir la carga computacional del algoritmo es necesario discretizar las features continuas y reducir los nivels de las \n",
    "    #       categóricas. Eso puede dar lugar a fenomenos como los del ejemplo abajo \n",
    "    #\n",
    "    #       Dataset ORIGINAL\n",
    "    #       | cont1  |  cat1  | target |\n",
    "    #       | 18     |  A     | 0.9    |\n",
    "    #       | 17     |  A     | 0.8    |\n",
    "    #       | 19     |  B     | 0.7    |\n",
    "    #       | 30     |  C     | 0.0    |\n",
    "    #       | 41     |  D     | 0.1    |\n",
    "    #       | 52     |  E     | 0.95   |\n",
    "    #\n",
    "    #\n",
    "    #       Dataset después de discretizar la variable cont1 y reducir los niveles de la variable cat1\n",
    "    #       | cont1  |  cat1  | target |\n",
    "    #       | 20     |  A     | 0.9    |\n",
    "    #       | 20     |  A     | 0.8    |\n",
    "    #       | 20     |  A     | 0.7    |\n",
    "    #       | 30     |  C     | 0.0    |\n",
    "    #       | 41     |  D     | 0.1    |\n",
    "    #       | 52     |  E     | 0.95   |\n",
    "    #\n",
    "    #       Como efecto colateral de la discretización por ejemplo la tupla (20, A) ahora tiene 3 targets diferentes (0.9, 0.8, 0.7)\n",
    "    #       Por eso es necesario calcular el valor promedio para cada tupla de features\n",
    "    cols = list(df.columns)\n",
    "    model_df: pd.DataFrame = (df\n",
    "                              .groupby([c for c in cols if c not in target_cols])\n",
    "                              .agg({c: (['count', 'mean'] if i == 0 else ['mean']) for i, c in enumerate(target_cols)})\n",
    "                              .reset_index())\n",
    "    model_df.columns = ['_'.join(col).strip() if col[1] != '' else col[0] for col in model_df.columns.values]\n",
    "    model_df = model_df.rename(columns={'_'.join([target_cols[0], 'mean']): target_cols[0], '_'.join([target_cols[0], 'count']): count_col})\n",
    "    if len(target_cols) > 1:\n",
    "        model_df = model_df.rename(columns={'_'.join([c, 'mean']): c for c in target_cols[1:]})\n",
    "        \n",
    "    \n",
    "    # Para poder luego realizar filtros más rápido pasamos cada columna a array de numpy\n",
    "    #  pandas --> dict: key=nombre columna, value=numpy.array de valores\n",
    "    model: Dict = {}\n",
    "    for c in model_df.columns:\n",
    "        model[c] = model_df[c].values\n",
    "        \n",
    "    # Pre-calculamos (los templates de) todas las coaliciones y sus pesos\n",
    "    coalitions: Dict = {}\n",
    "    coalitions_weights: Dict = {}\n",
    "    cols_names: List = df.columns\n",
    "    len_names: int = len(cols_names)\n",
    "    for c in [tmp_c for tmp_c in df.columns if tmp_c not in target_cols]:\n",
    "        coalitions[c] = get_coalitions(cols_names, c, target_cols)\n",
    "        coalitions_weights[c] = [1 / (scipy.special.comb(len_names - 1 - len(target_cols), len(coalition)) * (len_names-len(target_cols))) for coalition in coalitions[c]]\n",
    "    return model, coalitions, coalitions_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1de39f-a2f7-417a-83ee-f5af52e058cf",
   "metadata": {},
   "source": [
    "**FUNCIONES PARA FLUJO PARA EL CÁLCULO DE EXPLICABILIDAD LOCAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492250d4-afac-4b52-bf47-a7031be6dcf5",
   "metadata": {},
   "source": [
    "Estructura del flujo para el cálculo de la explicabilidad local\n",
    "\n",
    "**calculate_exact_shap_values**\n",
    "\n",
    "Recibe como entrada la información necesaria para generar la explicación local (la salida de la función train) y el dataframe a explicar.\n",
    "Recorre uno a uno los registros del dataframe a explicar y calcula los Shapley Value\n",
    "\n",
    "Salida:\n",
    "* $E[F(x)]$: el valor esperado del output del modelo estimado sobre el dataframe de train\n",
    "* Shapley value: Lista (un elemento para cada fila del dataframe a analizar) de listas (los shapley value de la fila, un valor por cada features)\n",
    "\n",
    "TODO: Ver si es posible optimizar (vectorizar) el flujo\n",
    "\n",
    "\n",
    "**coalition_contribution**\n",
    "\n",
    "Es la función que implementa el cálculo de \n",
    "\n",
    "$$\n",
    "\\frac{|S|! (D - 1 - |S|)!}{D!} \\left[ \\nu(S \\, \\cup \\{i\\}) - \\nu(S) \\right] = weight * \\left[coalition\\_worth(S \\cup i) - coalition\\_worth(S)\\right]\n",
    "$$\n",
    "\n",
    "siendo \n",
    "\n",
    "$$\n",
    "weight = \\frac{|S|! (D - 1 - |S|)!}{D!}\n",
    "$$ \n",
    "y \n",
    "\n",
    "$$\n",
    "coalition\\_worth(S) = \\nu(S)\n",
    "$$ \n",
    "\n",
    "**coalition_worth**\n",
    "\n",
    "Es la función que devuelve el valor de $\\nu(S)$\n",
    "\n",
    "La función comprueba en el diccionario de estado (el parámetro res_dict) si $\\nu(S)$ ha sido previamente calculado: si es el caso, se devuelve el valor previamente calculado, sino se estima $\\nu(S)$ llamando la función *get_worth* y se actualiza el diccionario de estado.\n",
    "\n",
    "**get_worth**\n",
    "\n",
    "Implementa el cálculo de $\\nu(S)$ como un filtro+media ponderada a partir de la información del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29787a67-e1e1-4615-ae94-ad36768a927b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_worth(coalition_features: Dict, model:Dict, target_cols:Optional[List[str]]=['target'], count_col:Optional[str]='count') -> List[float]:\n",
    "    \"\"\"Compute the coalition worth (\\nu(S))\n",
    "    \n",
    "    :param coalition_features: Dict containing the features of the coalition for which the \"worth\" has to be estimated. \n",
    "                               key: column name, value: value \n",
    "    :param model: Dict, model to use for the coalition \"worth\" estimation. key: column name, value: numpy.array of values\n",
    "    :param target_cols: List[str], list containing the names of the target columns, by default ['target']\n",
    "    :param count_col: str, name of the count column, by default 'count'\n",
    "    \n",
    "    :return: List[float], list of the \"worth\" of the current coalition, one for each target \n",
    "    \"\"\"\n",
    "    cond: np.array = np.ones_like(list(model.values())[0], dtype=np.bool8)\n",
    "    for k, v in coalition_features.items():\n",
    "        cond = (cond & (model[k] == v))\n",
    "    counts: np.array = model[count_col][cond]\n",
    "    return np.array([np.sum(counts * model[target_col][cond]) /  np.sum(counts)  for target_col in target_cols])\n",
    "\n",
    "def coalition_worth(x, coalition: List[str], model: Dict, res_dict: Dict, \n",
    "                    target_cols: Optional[List[str]]='target', count_col: Optional[str]='count') -> List[float]:\n",
    "    \"\"\"Return the coalition worth (\\nu(S))  \n",
    "    \n",
    "    :param x: Features to use for specialize the coalition template\n",
    "    :param model: Dict, model to use for the coalition \"worth\" estimation. key: column name, value: numpy.array of values\n",
    "    :param res_dict: Dict, containing the coalition worth already computed. key: str associated to the coalition, value: float\n",
    "    :param target_cols: List[str], list containing the names of the target columns, by default ['target']\n",
    "    :param count_col: str, name of the count column, by default 'count'\n",
    "    \n",
    "    :return: List[float], list of the \"worth\" of the current coalition, one for each target \n",
    "    \"\"\"\n",
    "    coalition_features: Dict = {c: x[c] if isinstance(x[c], str) else float(x[c]) for c in sorted(coalition)} \n",
    "    str_cf: str = str(coalition_features)\n",
    "    if res_dict.get(str_cf, None) is not None:\n",
    "        return res_dict[str_cf]\n",
    "    \n",
    "    to_ret: List[float] = get_worth(coalition_features, model, target_cols, count_col)\n",
    "    res_dict.update({str_cf: to_ret})\n",
    "    return to_ret\n",
    "            \n",
    "def coalition_contribution(x, col: str, coalition: List[str], weight: float, model: Dict, res_dict: Dict,  \n",
    "                           target_cols: Optional[List[str]]=['target'], count_col: Optional[str]='count') -> List[float]:\n",
    "    \"\"\"Compute the contribution of the current coalition to the Shapley value of the column  \"col\"\n",
    "    \n",
    "    :param x: Features to use for specialize the coalition template\n",
    "    :param coalition: List, coalition for which the contribution has to be computed\n",
    "    :param weight: float, coalition contribution weight (combinatorial weight)\n",
    "    :param model: Dict, model to use for the coalition \"worth\" estimation. key: column name, value: numpy.array of values\n",
    "    :param res_dict: Dict, containing the coalition worth already computed. key: str associated to the coalition, value: float\n",
    "    :param target_cols: List[str], list containing the names of the target columns, by default ['target']\n",
    "    :param count_col: str, name of the count column, by default 'count'\n",
    "    \n",
    "    :return: List[float], list of the current coalition contribution to the Shapley Value, one for each target\n",
    "    \"\"\"\n",
    "    return weight * (coalition_worth(x, coalition + [col], model, res_dict, target_cols, count_col) - \n",
    "                     coalition_worth(x, coalition, model, res_dict, target_cols, count_col))\n",
    "            \n",
    "def calculate_exact_shap_values(df_2_ana: pd.DataFrame, model: Dict, coalitions_dict: dict, coalitions_weights_dict: Dict, \n",
    "                                res_dict: Dict, target_cols: Optional[List[str]]=['target'], \n",
    "                                count_col: Optional[str]='count')->Tuple[float, List[List[float]]]:\n",
    "    \"\"\"General flux to compute the local explaination for a given dataframe (df_2_ana) of the type [features, target]\n",
    "    \n",
    "    :param df_2_ana: pd.DataFrame, dataframe to explain\n",
    "    :param model: Dict, model to use for the coalition \"worth\" estimation. key: column name, value: numpy.array of values\n",
    "                  Output of the train function.      \n",
    "    :param coalitions_dict: Dict, the coalitions compatible with each dataframe column. Key: column name, value: list of coalitions \n",
    "    :param coalitions_weights_dict: Dict, the combinatorial weights of each coalitions. Key: column name, value: list of weights (one for each\n",
    "                                    coalitions)\n",
    "    :param res_dict: Dict containing the coalition worth already computed. key: str associated to the coalition, value: float \n",
    "    :param target_cols: List[str], list containing the names of the target columns, by default ['target']\n",
    "    :param count_col: str, name of the count column, by default 'count'\n",
    "    \n",
    "    :return: Tuple, \n",
    "       - E[F(X)] --> the expected value of the model prediction (baseline with respect the local explaination is computed)\n",
    "       - Shapley Values --> List (an element for each row of the dataframe to analyze) of List (an element for each DataFrame  column) \n",
    "                            of float (Shapley value of the column)\n",
    "    \"\"\"\n",
    "    shap_values_list: List = []\n",
    "    for i in range(df_2_ana.shape[0]):\n",
    "        # Only for trace/log\n",
    "        if (i > 0) and (i % 100 == 0):\n",
    "            print('Progress: row {}'.format(i))\n",
    "        \n",
    "        # XAI: para cada fila, calculo de los shapley value    \n",
    "        x = df_2_ana.iloc[i]\n",
    "        shap_values: List = []\n",
    "        for col in df_2_ana.columns:\n",
    "            # For sobre las columnas: hay que calcular un shapley value para cada columna\n",
    "            if col in target_cols:\n",
    "                continue\n",
    "            shap_value =  np.sum(np.array([coalition_contribution(x, col, coalition, weight, model, res_dict, target_cols, count_col) \n",
    "                                                  for coalition, weight in zip(coalitions_dict[col], coalitions_weights_dict[col])]), \n",
    "                                        axis=0)\n",
    "            shap_values.append(shap_value)\n",
    "        shap_values_list.append(shap_values)\n",
    "    return res_dict[str({})], shap_values_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a01bc-9cc8-4aca-923e-2f79af112917",
   "metadata": {},
   "source": [
    "**MAIN**\n",
    "\n",
    "Función que simula el flujo. \n",
    "\n",
    "NOTA: de momento para problema de regresión o problema de clasificación binaria y asumiendo un número de features <=20. \n",
    "En el caso de clasificación multiclase, sería necesario previamente transformar el problema multiclase en N problemas binarios e iterar N veces el flujo de esta función.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc8863f-9e38-4733-a32c-3222d4db8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df_2_analyze: pd.DataFrame, df_train: pd.DataFrame, state_dict: Optional[Dict]={}, target_col: str= 'target') -> Tuple:\n",
    "    \"\"\"Main flux for the case of regression or binary classification and number of features <= 20\n",
    "    \n",
    "    :param df_2_analyze: pd.DataFrame with the model prediction to explain (local explaination)\n",
    "    :param df_train: pd.DataFrame to use for \"training\" the local explaination algorithm\n",
    "    :param state_dict: Dict, Optional state containing the coalition worths already computed\n",
    "    \n",
    "    :return: Tuple\n",
    "        - E[F(X)] --> the expected value of the model prediction (baseline with respect the local explaination is computed)\n",
    "        - Shapley Values --> np.array of shape [len(df_2_analyze), len(features of df_2_analyze)] containing the Shapley values\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    start = time.time()\n",
    "    target_cols = [c for c in cols if target_col in c]\n",
    "    model, coalitions_dict, coalitions_weights_dict = train(df_train, target_cols)\n",
    "    print('Training time: {}'.format(time.time() - start))\n",
    "    \n",
    "    print(model.keys())\n",
    "\n",
    "    # Local explaination\n",
    "    phi0, shap = calculate_exact_shap_values(df_2_analyze, model, coalitions_dict, coalitions_weights_dict, state_dict, target_cols=target_cols)\n",
    "    print('Computation time: {}'.format(time.time() - start))\n",
    "\n",
    "    return phi0, np.array(shap), target_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4870a5-73e5-43a7-ae44-cf5fccdb49e7",
   "metadata": {},
   "source": [
    "Por curiosidad, estudio del comportamiento de $\\frac{|S|! (D - 1 - |S|)!}{D!}$ para el caso del titanic, es decir 8 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0535bba-8c7a-47ee-a2e0-207df7cde0ae",
   "metadata": {},
   "source": [
    "## Titanic con TEF-SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02458f0d-6bf9-4877-8512-b436195ad1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.03291153907775879\n",
      "dict_keys(['age', 'fare', 'family_size', 'embarked', 'sex', 'title', 'is_alone', 'survived', 'count', 'target_1.0', 'target_2.0', 'target_3.0'])\n",
      "Progress: row 100\n",
      "Progress: row 200\n",
      "Progress: row 300\n",
      "Progress: row 400\n",
      "Progress: row 500\n",
      "Computation time: 36.17792248725891\n"
     ]
    }
   ],
   "source": [
    "# Lanzamos el script train+explain para titanic\n",
    "phi0, shap_values, target_cols =  main(df_2_explain, df_2_explain, state_dict={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bade83fd-cc9a-4243-bee0-871845bd9840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561, 8, 3)\n",
      "561\n",
      "[0.29072589 0.23059517 0.47867895]\n",
      "(561, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(shap_values.shape)\n",
    "print(len(df_2_explain))\n",
    "print(phi0)\n",
    "\n",
    "reconstr = np.round(phi0 + np.sum(shap_values, axis=1), 6)\n",
    "\n",
    "print(reconstr.shape)\n",
    "reconstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6554876c-7ad6-46d9-8801-de8dec6bfecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions to explain: 561\n",
      "Detected Target 1: [159 116 256]\n",
      "Detected Target 0: [394 418 279]\n",
      "Detected Target 0 or 1: [553 534 535]\n",
      "Discrepancias (predicción modelo original != predicción SHAP) detectadas target_1.0: 0\n",
      "Discrepancias (predicción modelo original != predicción SHAP) detectadas target_2.0: 0\n",
      "Discrepancias (predicción modelo original != predicción SHAP) detectadas target_3.0: 0\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos la calidad de la accuracy local\n",
    "reconstr = np.round(phi0 + np.sum(shap_values, axis=1), 6)\n",
    "print('Total predictions to explain: {}'.format(len(df_2_explain)))\n",
    "print('Detected Target 1: {}'.format(sum(reconstr==1)))\n",
    "print('Detected Target 0: {}'.format(sum(reconstr==0)))\n",
    "print('Detected Target 0 or 1: {}'.format(sum(reconstr==1) + sum(reconstr==0)))\n",
    "\n",
    "counts=[0 for _ in range(len(target_cols))]\n",
    "eps_error = 0.000001\n",
    "for i in range(len(df_2_explain)):\n",
    "    for j, tar in enumerate(target_cols):\n",
    "        if abs(reconstr[i, j] - df_2_explain.iloc[i][tar]) > eps_error:\n",
    "            counts[j] += 1\n",
    "for j, tar in enumerate(target_cols): \n",
    "    print('Discrepancias (predicción modelo original != predicción SHAP) detectadas {}: {}'.format(tar, counts[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c562c-2fde-41a2-8c0c-58d4216ce058",
   "metadata": {},
   "source": [
    "**Explicabilidad global**\n",
    "\n",
    "Para la explicabilidad global, usamos el mismo truco usado en la librería SHAP\n",
    "\n",
    "explicabilidad global = mean(abs(explicabilidad local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310a1df-d335-4bff-b2bf-5c10ceafcddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicabilidad global\n",
    "global_imp = np.abs(shap_values).mean(axis=0)\n",
    "global_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed7b84-c7d0-41bd-9c31-ab7e8cd19d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos los resultados\n",
    "plt.figure(figsize=(10, 7))\n",
    "x_axis = np.arange(len([c for c in df_2_explain.columns if c not in target_cols]))\n",
    "for j in range(len(target_cols)): \n",
    "    list1, list2 = zip(*sorted(zip(global_imp[:, j].tolist(), [c for c in df_2_explain.columns if c not in target_cols])))\n",
    "    plt.bar(x_axis - len(target_cols)/4*(2-0.2)/(2*len(target_cols)) + j*(2-0.2)/(2*len(target_cols)), global_imp[:, j].tolist(), width=(2-0.2)/(2*len(target_cols)), label = target_cols[j])\n",
    "\n",
    "plt.xticks(x_axis, [c for c in df_2_explain.columns if c not in target_cols])\n",
    "plt.ylabel('mean(|Shapley|)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a078b-4ac0-48d6-89ea-d9132acda505",
   "metadata": {},
   "source": [
    "**Explicabilidad local**\n",
    "\n",
    "Visualizamos algunos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c09d272-e5c2-4c2b-bb57-51792ea90ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(40, 70):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    x_axis = np.arange(len([c for c in df_2_explain.columns if c not in target_cols]))\n",
    "    for j in range(len(target_cols)): \n",
    "        plt.bar(x_axis - len(target_cols)/4*(2-0.2)/(2*len(target_cols)) + j*(2-0.2)/(2*len(target_cols)), shap_values[i, :, j].tolist(), width=(2-0.2)/(2*len(target_cols)), label = target_cols[j])\n",
    "\n",
    "    plt.xticks(x_axis, [c + '_{}'.format(df_2_explain[c].values[i]) for c in df_2_explain.columns if c not in target_cols])\n",
    "    plt.ylabel('mean(|Shapley|)')\n",
    "    plt.legend()\n",
    "    plt.title([c + '_{}'.format(df_2_explain[c].values[i]) for c in df_2_explain.columns if c in target_cols])\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97facd4e-4f1a-48c5-9f9f-06fdf5adfda4",
   "metadata": {},
   "source": [
    "# Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e6844-1bec-4d0d-99e7-12ad0e25de9c",
   "metadata": {},
   "source": [
    "Objetivo: mostrar como la correlación entre variables puede dificultar la interpretación de la explicabilidad local proporcionada por SHAP\n",
    "\n",
    "Explicación: De forma naive, si dos features f1 y f2 correlan fortemente solo la explicación de f1+f2 (la suma de los Shapley Value de f1 y f2) tiene sentido.  Si f1 + f2 = K, lo importante es K como se reparte K entre f1 y f2 dependerá la implementación de SHap, de los datos utilizados para estimar la contribución de las coaliciones, etc.\n",
    "\n",
    "Vamos a intentar ponerlo en evidencia mostrando como la explicabilidad obtenida por SHAP y TEF-SHAP es equivalente SOLO cuando se \"agrupan/suman\" las variables que correlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2489f-ede9-498f-94fe-0983c67542aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1, list2 = zip(*sorted(zip(global_imp.tolist(), tmp.columns[:-1])))\n",
    "plt.barh([c for c in list2], list1, label='classic')\n",
    "for i, v in enumerate(list1):\n",
    "    plt.text(v + 0.00, i - .1, str(round(v, 3)), color='blue', fontweight='bold')\n",
    "#print(tmp.iloc[i])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ShapExplainer.plot_shap_global_xai(shap_lib_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca1489-f681-4b37-b49c-2f1a497ea5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cols = ['sex', 'pclass', 'title', 'age', 'embarked', 'fare', 'family_size', 'is_alone']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ee82b-143c-4827-92bb-eb5afcedc650",
   "metadata": {},
   "source": [
    "### \"Correlación\" entre variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794b1df-4bcb-412d-b8a2-2228e274ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ac512-9cae-4506-8860-61c4821ef2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(tmp.columns)[:-1]\n",
    "cols1 = cols\n",
    "cols2 = cols\n",
    "\n",
    "cat_var_prod = list(product(cols1,cols2, repeat = 1))\n",
    "print(len(cat_var_prod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710706e-351b-42ff-ac6b-1ce81a0f7431",
   "metadata": {},
   "source": [
    "**Cramer v**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n",
    "\n",
    "Código: https://github.com/shakedzy/dython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120538cd-d908-4e6e-836c-974a070b7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234b6d7-37ec-4333-a379-d156ea40b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "c_result = []\n",
    "for i in cat_var_prod:\n",
    "    c_result.append((i[0],i[1],cramers_v(tmp[i[0]], tmp[i[1]])))\n",
    "        \n",
    "cramer_output = pd.DataFrame(c_result, columns = ['var1', 'var2', 'coeff'])\n",
    "## Using pivot function to convert the above DataFrame into a crosstab\n",
    "final_df = cramer_output.pivot(index='var1', columns='var2', values='coeff')\n",
    "\n",
    "sns.heatmap(final_df, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf58fb-9534-4e4d-b943-3e55a0745b12",
   "metadata": {},
   "source": [
    "**Theils u**\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Uncertainty_coefficient\n",
    "\n",
    "Código: https://github.com/shakedzy/dython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b8575-e697-404b-a2b9-a450fa187143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "_REPLACE = \"replace\"\n",
    "_DROP = \"drop\"\n",
    "_DEFAULT_REPLACE_VALUE = 0.0\n",
    "\n",
    "def replace_nan_with_value(x, y, value):\n",
    "    x = np.array(\n",
    "        [v if v == v and v is not None else value for v in x]\n",
    "    )  # NaN != NaN\n",
    "    y = np.array([v if v == v and v is not None else value for v in y])\n",
    "    return x, y\n",
    "\n",
    "def conditional_entropy(\n",
    "    x,\n",
    "    y,\n",
    "    nan_strategy=_REPLACE,\n",
    "    nan_replace_value=_DEFAULT_REPLACE_VALUE,\n",
    "    log_base: float = math.e,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the conditional entropy of x given y: S(x|y)\n",
    "    Wikipedia: https://en.wikipedia.org/wiki/Conditional_entropy\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : list / NumPy ndarray / Pandas Series\n",
    "        A sequence of measurements\n",
    "    y : list / NumPy ndarray / Pandas Series\n",
    "        A sequence of measurements\n",
    "    nan_strategy : string, default = 'replace'\n",
    "        How to handle missing values: can be either 'drop' to remove samples\n",
    "        with missing values, or 'replace' to replace all missing values with\n",
    "        the nan_replace_value. Missing values are None and np.nan.\n",
    "    nan_replace_value : any, default = 0.0\n",
    "        The value used to replace missing values with. Only applicable when\n",
    "        nan_strategy is set to 'replace'.\n",
    "    log_base: float, default = e\n",
    "        specifying base for calculating entropy. Default is base e.\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "    \"\"\"\n",
    "    if nan_strategy == _REPLACE:\n",
    "        x, y = replace_nan_with_value(x, y, nan_replace_value)\n",
    "    elif nan_strategy == _DROP:\n",
    "        x, y = remove_incomplete_samples(x, y)\n",
    "    y_counter = Counter(y)\n",
    "    xy_counter = Counter(list(zip(x, y)))\n",
    "    total_occurrences = sum(y_counter.values())\n",
    "    entropy = 0.0\n",
    "    for xy in xy_counter.keys():\n",
    "        p_xy = xy_counter[xy] / total_occurrences\n",
    "        p_y = y_counter[xy[1]] / total_occurrences\n",
    "        entropy += p_xy * math.log(p_y / p_xy, log_base)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def theils_u(x, y):\n",
    "    s_xy = conditional_entropy(x,y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
    "    s_x = ss.entropy(p_x)\n",
    "    if s_x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return (s_x - s_xy) / s_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c7ef8e-c490-4e4b-aab5-445c31efde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "t_result = []\n",
    "for i in cat_var_prod:\n",
    "    t_result.append((i[0],i[1],theils_u(tmp[i[0]], tmp[i[1]])))\n",
    "        \n",
    "theils_output = pd.DataFrame(t_result, columns = ['var1', 'var2', 'coeff'])\n",
    "## Using pivot function to convert the above DataFrame into a crosstab\n",
    "final_df = theils_output.pivot(index='var1', columns='var2', values='coeff')\n",
    "\n",
    "sns.heatmap(final_df, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33269a2-a143-454c-ae78-f1abf3fced9c",
   "metadata": {},
   "source": [
    "**Chi2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c879d27-c2b6-4c2a-8b4c-1d748530944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2\n",
    "result = []\n",
    "dofs = []\n",
    "prob = 0.95\n",
    "for i in cat_var_prod:\n",
    "    if i[0] != i[1]:\n",
    "        stat, p, dof, expected = list(ss.chi2_contingency(pd.crosstab(tmp[i[0]], tmp[i[1]])))\n",
    "        #critical = ss.chi2.ppf(prob, dof)\n",
    "        result.append((i[0],i[1],p))\n",
    "        #result.append((i[0],i[1],abs(stat) > critical))\n",
    "        dofs.append((i[0],i[1],dof))\n",
    "\n",
    "chi_test_output = pd.DataFrame(result, columns = ['var1', 'var2', 'coeff'])\n",
    "dof_chi_test_output = pd.DataFrame(dofs, columns = ['var1', 'var2', 'coeff'])\n",
    "## Using pivot function to convert the above DataFrame into a crosstab\n",
    "final_df = chi_test_output.pivot(index='var1', columns='var2', values='coeff')\n",
    "sns.heatmap(final_df, annot=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869bd6f-7a0b-4760-9821-a49472e5479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_var1 = sex + title\n",
    "# new_var2 = pclass + fare\n",
    "# new_var3 = is_alone + family_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa514a7-d586-47d7-880f-810723ba30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = [\n",
    "    ['sex_title', 0.241],\n",
    "    ['pclass_fare', 0.159],\n",
    "    ['is_alone_family_size', 0.076],\n",
    "    ['age', 0.081],\n",
    "    ['embarked', 0.06]\n",
    "]\n",
    "\n",
    "shap_ref = [\n",
    "    ['sex_title', 0.31],\n",
    "    ['pclass_fare', 0.18],\n",
    "    ['is_alone_family_size', 0.06],\n",
    "    ['age', 0.07],\n",
    "    ['embarked', 0.06]\n",
    "]\n",
    "\n",
    "listA, listB = zip(*ours)\n",
    "listB, listA = zip(*sorted(zip(listB, listA)))\n",
    "plt.barh([c for c in listA], listB, label='classic')\n",
    "for i, v in enumerate(listB):\n",
    "    plt.text(v + 0.00, i - .1, str(round(v, 3)), color='blue', fontweight='bold')\n",
    "#print(tmp.iloc[i])\n",
    "plt.legend()\n",
    "plt.title('NEW ALGORITHM')\n",
    "plt.show()\n",
    "\n",
    "listA, listB = zip(*shap_ref)\n",
    "listB, listA = zip(*sorted(zip(listB, listA)))\n",
    "plt.barh([c for c in listA], listB, label='classic')\n",
    "for i, v in enumerate(listB):\n",
    "    plt.text(v + 0.001, i - .1, str(round(v, 3)), color='blue', fontweight='bold')\n",
    "#print(tmp.iloc[i])\n",
    "plt.legend()\n",
    "plt.title('SHAP LIB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab21f1-0d12-44d1-a28a-82268b282b8d",
   "metadata": {},
   "source": [
    "## TEF-SHAP: PERMUTATION BASED ESTIMATION\n",
    "\n",
    "Implementación para intentar escalar a > 20 columnas\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43198d-68de-4f4a-a806-6891c4b4f019",
   "metadata": {},
   "source": [
    "## TEF-SHAP: REGRESSION BASED ESTIMATION\n",
    "\n",
    "Implementación para intentar escalar a > 20 columnas\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ac9a3-35e1-44ae-af2c-a28a5193b382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grap_nn",
   "language": "python",
   "name": "grap_nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
